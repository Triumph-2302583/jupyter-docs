
Open In Colab

import pandas as pd

dataset = pd.read_csv("https://raw.githubusercontent.com/sudarshan-koirala/Logistic-Regression-for-Titanic-Dataset/master/Train_Titanic.csv")
     

rows, cols = dataset.shape
print(f"There are {rows} rows and {cols} columns in the dataset.")

     
There are 891 rows and 12 columns in the dataset.

dataset.shape
     
(891, 12)

dataset.info()
     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB

dataset.head()
     
PassengerId	Survived	Pclass	Name	Sex	Age	SibSp	Parch	Ticket	Fare	Cabin	Embarked
0	1	0	3	Braund, Mr. Owen Harris	male	22.0	1	0	A/5 21171	7.2500	NaN	S
1	2	1	1	Cumings, Mrs. John Bradley (Florence Briggs Th...	female	38.0	1	0	PC 17599	71.2833	C85	C
2	3	1	3	Heikkinen, Miss. Laina	female	26.0	0	0	STON/O2. 3101282	7.9250	NaN	S
3	4	1	1	Futrelle, Mrs. Jacques Heath (Lily May Peel)	female	35.0	1	0	113803	53.1000	C123	S
4	5	0	3	Allen, Mr. William Henry	male	35.0	0	0	373450	8.0500	NaN	S
Exploratory Data Analysis
Missing value check

missing_values = dataset.isnull().sum()
print(missing_values)

     
PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64

dataset.describe()
     
PassengerId	Survived	Pclass	Age	SibSp	Parch	Fare
count	891.000000	891.000000	891.000000	714.000000	891.000000	891.000000	891.000000
mean	446.000000	0.383838	2.308642	29.699118	0.523008	0.381594	32.204208
std	257.353842	0.486592	0.836071	14.526497	1.102743	0.806057	49.693429
min	1.000000	0.000000	1.000000	0.420000	0.000000	0.000000	0.000000
25%	223.500000	0.000000	2.000000	20.125000	0.000000	0.000000	7.910400
50%	446.000000	0.000000	3.000000	28.000000	0.000000	0.000000	14.454200
75%	668.500000	1.000000	3.000000	38.000000	1.000000	0.000000	31.000000
max	891.000000	1.000000	3.000000	80.000000	8.000000	6.000000	512.329200

import matplotlib.pyplot as plt

plt.hist(dataset['Age'])
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Distribution of Age')
plt.show()

     


plt.scatter(dataset['Age'], dataset['Fare'])
plt.xlabel('Age')
plt.ylabel('Fare')
plt.title('Relationship between Age and Fare')
plt.show()

     


survived_counts = dataset['Survived'].value_counts()
survived_counts.plot(kind='bar')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.title('Distribution of Survived')
plt.xticks([0, 1], ['No', 'Yes'])
plt.show()

     


survived_sex_counts = dataset.groupby(['Survived', 'Sex']).size().unstack()
survived_sex_counts.plot(kind='bar', stacked=True)
plt.xlabel('Survived')
plt.ylabel('Count')
plt.title('Distribution of Survived and Sex')
plt.xticks([0, 1], ['No', 'Yes'])
plt.show()

     


import seaborn as sns

sns.distplot(dataset['SibSp'], kde=False, bins=8)
plt.xlabel('Number of Siblings/Spouses')
plt.ylabel('Count')
plt.title('Distribution of SibSp')
plt.show()

     
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)


survived_pclass_counts = dataset.groupby(['Survived', 'Pclass']).size().unstack()
survived_pclass_counts.plot(kind='bar', stacked=False)
plt.xlabel('Survived')
plt.ylabel('Count')
plt.title('Distribution of Survived by Pclass')
plt.xticks([0, 1], ['No', 'Yes'])
plt.legend(title='Pclass')
plt.show()

     


import seaborn as sns

missing = dataset.isnull()
sns.heatmap(missing, cbar=False)
plt.title('Missing Values')
plt.show()

     


def impute_age_by_pclass(df):
    age_by_pclass = df.groupby('Pclass')['Age'].mean()

    for pclass, age in age_by_pclass.iteritems():
        df.loc[(df['Pclass'] == pclass) & (df['Age'].isnull()), 'Age'] = age

    return df

     

dataset = impute_age_by_pclass(dataset)
import seaborn as sns

missing = dataset.isnull()
sns.heatmap(missing, cbar=False)
plt.title('Missing Values')
plt.show()
     


dataset = dataset.drop('Cabin', axis=1)

     

dataset.head()
     
PassengerId	Survived	Pclass	Name	Sex	Age	SibSp	Parch	Ticket	Fare	Embarked
0	1	0	3	Braund, Mr. Owen Harris	male	22.0	1	0	A/5 21171	7.2500	S
1	2	1	1	Cumings, Mrs. John Bradley (Florence Briggs Th...	female	38.0	1	0	PC 17599	71.2833	C
2	3	1	3	Heikkinen, Miss. Laina	female	26.0	0	0	STON/O2. 3101282	7.9250	S
3	4	1	1	Futrelle, Mrs. Jacques Heath (Lily May Peel)	female	35.0	1	0	113803	53.1000	S
4	5	0	3	Allen, Mr. William Henry	male	35.0	0	0	373450	8.0500	S

dataset = pd.get_dummies(dataset, columns=['Sex', 'Embarked'], drop_first=True)
dataset
     
PassengerId	Survived	Pclass	Name	Age	SibSp	Parch	Ticket	Fare	Sex_male	Embarked_Q	Embarked_S
0	1	0	3	Braund, Mr. Owen Harris	22.00000	1	0	A/5 21171	7.2500	1	0	1
1	2	1	1	Cumings, Mrs. John Bradley (Florence Briggs Th...	38.00000	1	0	PC 17599	71.2833	0	0	0
2	3	1	3	Heikkinen, Miss. Laina	26.00000	0	0	STON/O2. 3101282	7.9250	0	0	1
3	4	1	1	Futrelle, Mrs. Jacques Heath (Lily May Peel)	35.00000	1	0	113803	53.1000	0	0	1
4	5	0	3	Allen, Mr. William Henry	35.00000	0	0	373450	8.0500	1	0	1
...	...	...	...	...	...	...	...	...	...	...	...	...
886	887	0	2	Montvila, Rev. Juozas	27.00000	0	0	211536	13.0000	1	0	1
887	888	1	1	Graham, Miss. Margaret Edith	19.00000	0	0	112053	30.0000	0	0	1
888	889	0	3	Johnston, Miss. Catherine Helen "Carrie"	25.14062	1	2	W./C. 6607	23.4500	0	0	1
889	890	1	1	Behr, Mr. Karl Howell	26.00000	0	0	111369	30.0000	1	0	0
890	891	0	3	Dooley, Mr. Patrick	32.00000	0	0	370376	7.7500	1	1	0
891 rows Ã— 12 columns


dataset = dataset.drop(['Name', 'Ticket'], axis=1)
dataset.head()
     
PassengerId	Survived	Pclass	Age	SibSp	Parch	Fare	Sex_male	Embarked_Q	Embarked_S
0	1	0	3	22.0	1	0	7.2500	1	0	1
1	2	1	1	38.0	1	0	71.2833	0	0	0
2	3	1	3	26.0	0	0	7.9250	0	0	1
3	4	1	1	35.0	1	0	53.1000	0	0	1
4	5	0	3	35.0	0	0	8.0500	1	0	1
ML part

from sklearn.model_selection import train_test_split

X = dataset.drop('Survived', axis=1)
y = dataset['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

     

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
     
/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
LogisticRegression()

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

     
Accuracy: 0.7653631284916201

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# make predictions on the testing data
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

# print the results
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC AUC score:", roc_auc)

     
Accuracy: 0.7653631284916201
Precision: 0.7352941176470589
Recall: 0.6756756756756757
F1-score: 0.7042253521126761
ROC AUC score: 0.8375804375804377

from sklearn.metrics import confusion_matrix
import seaborn as sns

# make predictions on the testing data
y_pred = model.predict(X_test)

# compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# plot the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

     


from sklearn.preprocessing import StandardScaler

# initialize the scaler
scaler = StandardScaler()

# fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# transform the testing data
X_test_scaled = scaler.transform(X_test)
     

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train_scaled, y_train)
     
LogisticRegression()

y_pred = model.predict(X_test_scaled)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
     
Accuracy: 0.8156424581005587


     
